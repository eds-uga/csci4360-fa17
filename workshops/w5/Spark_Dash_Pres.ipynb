{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll talk a little bit about the Spark's precursor, Hadoop, and then we'll discuss the advantages and utility of Spark on top of Hadoop.\n",
    "\n",
    "Next, we'll discuss what Dash offers in conjunction with Spark. \n",
    "\n",
    "Finally, we will implement a simple Spark application using the PySpark API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the Beginning, There was Hadoop... \n",
    "\n",
    "![title](data/pic/hadoop-logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Hadoop - an open source software framework for reliable,scalable, distributed computing on commodity hardware  \n",
    "\n",
    "1. Main modules:\n",
    "    * Hadoop Common - main Hadoop Utilities\n",
    "    * HDFS - **H**adoop **D**istributed **F**ile **S**ystem\n",
    "    * Hadoop YARN - a task scheduling / cluster resource management module\n",
    "    * Hadoop MAP/REDUCE - paralell computing paradigm for big data anaylytics \n",
    "2. Emphasis on reliability and scalability\n",
    "    * Pervasive Assumption: node failure is the rule, not the exception \n",
    "    * Task / Cluster management is performed automatically, under the hood\n",
    "3. YARN is rack aware \n",
    "    * Tasks are scheduled either on a node where the data is already housed, or preference is given to a node on the same rack \n",
    "    * Reduces overall traffic between racks, increasing throughput "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Analytics with Hadoop \n",
    "1. Store the data using the HDFS \n",
    "2. Define a `Mapper` class (user defined class extends `Mapper`) which implements a `map` function\n",
    "    * The `map` function takes in a pair, `<k1,v1>` , and maps it to a new value, `<k2,v2>`\n",
    "    * The prototypical example is a word count program : map a document to a set of `<k,v>=<w,c>` pairs\n",
    "        * The document is distributed as chunks throughout the file system\n",
    "            * Each chunk of the document becomes a value, v, associated with a key, k, in a `<k,v>` pair\n",
    "        * Split each value into tokens (i.e. - words) associated with an iterator, iter \n",
    "        * For every word in iter, w, create a `<k,v>=<w,1>` pair \n",
    "        * Output a multi-set of `<w,1>` pairs for the chunk\n",
    "            * We now have one multiset for each chunk of the document\n",
    "        * Collect the multisets into a single multiset \n",
    "3. Define a *Reducer* class which impelements a `reduce` function \n",
    "    * The *reduce* function reduces the output from the mapping to a more meaningful state\n",
    "        * Reduce the multiset to a set of `<w,c>` pairs by summing all *v* with the same *k*   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Map/Reduce Example Implementation: \n",
    "\n",
    "    public static class TokenizerMapper\n",
    "           extends Mapper<Object, Text, Text, IntWritable>{\n",
    "\n",
    "         private final static IntWritable one = new IntWritable(1);\n",
    "         private Text word = new Text();\n",
    "\n",
    "         public void map(Object key, Text value, Context context\n",
    "                    ) throws IOException, InterruptedException {\n",
    "             StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "             while (itr.hasMoreTokens()) {\n",
    "                 word.set(itr.nextToken());\n",
    "                 context.write(word, one);\n",
    "             }\n",
    "         }\n",
    "    }\n",
    "\n",
    "    public static class IntSumReducer\n",
    "         extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "         private IntWritable result = new IntWritable();\n",
    "\n",
    "        public void reduce(Text key, Iterable<IntWritable> values, Context context\n",
    "                    ) throws IOException, InterruptedException {\n",
    "            int sum = 0;\n",
    "            for (IntWritable val : values) {\n",
    "            sum += val.get();\n",
    "            }\n",
    "        result.set(sum);\n",
    "        context.write(key, result);\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/pic/MapReduce_Work_Structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And Lo! the villagers did rejoice! \n",
    "\n",
    "![title](data/pic/Rejoice.jpg)\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ...for a time...\n",
    "![title](data/pic/Simpsons.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Problems with Hadoop MAP/REDUCE\n",
    "\n",
    "    1. Every application has to be pidgeon-holed into the MAP/REDUCE paradigm\n",
    "    2. Studies showed that up to 90% of computation wall time was spent in file I/O\n",
    "    3. Iterative algorithms were especially slow when implemented with MAP/REDUCE\n",
    "        * The application may bottle neck due to a low number of particularly slow nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/pic/BetterWay.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter Spark\n",
    "\n",
    "![title](data/pic/Spark.png)\n",
    "\n",
    " * Originally developed by Matei Zaharia and researchers at UC Berkley's AMP Lab\n",
    " * Donated to the Apache Software Foundation in 2013\n",
    " \n",
    " ## RDD - **R**esilient **Distributed** **Dataset**\n",
    " \n",
    " * The main data structure for Spark applications\n",
    " * Fault tolerant multi-set of data partitions available to a computing cluster in shared memory \n",
    " * Requires cluster management and distributed file system\n",
    "     * Supported cluster management includes Spark native, Hadoop YARN, and Apache Mesos\n",
    "     * Several supported DFS:\n",
    "         * HDFS\n",
    "         * MAP-R FDS \n",
    "         * Cassandra \n",
    "         * ...\n",
    "     * RDDs can be created by any storage source supported by Hadoop\n",
    "### In memory processing and a more diverse API are its main benefits over MAP/REDUCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Operations in Hadoop MAP/REDUCE:\n",
    "![title](data/pic/iterative_operations_on_mapreduce.jpg)\n",
    "\n",
    "### versus \n",
    "\n",
    "## Iterative Operations in Spark :\n",
    "![title](data/pic/iterative_operations_on_spark_rdd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Interactive Operations in Hadoop MAP/REDUCE:\n",
    "![title](data/pic/interactive_operations_on_mapreduce.jpg)\n",
    "\n",
    "### versus\n",
    "\n",
    "## Interactive Operations in Spark:\n",
    "![title](data/pic/interactive_operations_on_spark_rdd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark : a Python API for programming spark applications \n",
    "    * Originally Spark was written in Scala and most Spark applications were written in either Scala or Java\n",
    "    * Eventually support was extended with APIs for Python and R \n",
    "    * We are going to work with the Python API : PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SparkShell\n",
    "Spark has an REPL interactive shell called the SparkShell\n",
    "![title](data/pic/SparkShell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a simple Spark application...\n",
    "\n",
    "This piece of the presentation borrows heavily from a July 2017 DataCamp tutorial on machine learning and PySpark. Visit the following URL to see the original tutorial: \n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning#gs.Y3MIPIY\n",
    "\n",
    "## Data: California Realestate \n",
    "\n",
    "We're going to explore some data and run some iterative algorithms on it. You can find the data here: http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html \n",
    "\n",
    "Download the `cal_housing.tgz` tar ball at the bottom of the page and extract it somewhere obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to use the **findspark** package inorder to be able to locate the various Spark packages that we need (i.e. - pyspark, etc.) You can download and install findspark using *pip* as I did, or I'm sure *anaconda* will also work. There are probably other ways of making sure you can locate the packages you need as well, but this was the simplest and most straight forward I found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This is only necessary in the Jupyter Notebook. You should be able to import the necessary packages in a regular Python script without using *findspark*\n",
    "\n",
    "The SparkSession is the entry point for any Spark application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new **SparkSession** through the **builder** attribute and the **getOrCreate()** method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"LinearRegressionModel\")\\\n",
    "        .config(\"spark.executor.memory\",\"1gb\")\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the *master* and *appName* attributes isn't actually important or critical in this introduction, nor is configuring the memory options for the executor. I've included here for the sake of thoroughness.\n",
    "\n",
    "**NOTE:** If you find the Spark tutorial on the Spark documentation web page it includes the following line of code: \n",
    "\n",
    "    spark = SparkSession.builder().appName(appName).master(master).getOrCreate()\n",
    "\n",
    "That does not work. You must pass a string as an argument to the appName() and master() methods. \n",
    "\n",
    "Moving on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can create a couple of RDDs: one with the data and another with the domain information, the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('data/CaliforniaHousing/cal_housing.data')\n",
    "header = sc.textFile('data/CaliforniaHousing/cal_housing.domain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of what allows for some of the speed-up in Spark applications is that Spark evaluations are mostly lazy evals. So executing the following line of code isn't very useful: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/CaliforniaHousing/cal_housing.domain MapPartitionsRDD[33] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we have to take an action on the rdd, such as `collect()` to materialize the data represented by the rdd abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude: continuous.',\n",
       " 'latitude: continuous.',\n",
       " 'housingMedianAge: continuous. ',\n",
       " 'totalRooms: continuous. ',\n",
       " 'totalBedrooms: continuous. ',\n",
       " 'population: continuous. ',\n",
       " 'households: continuous. ',\n",
       " 'medianIncome: continuous. ',\n",
       " 'medianHouseValue: continuous. ']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**NOTE:** `collect()` is a pretty dangerous action: if the RDD is especially large then your executor you may run out of RAM and your application will crash. If you're using especially large data and you just want a peak at it to try to suss out its structure, then try `take()` or `first()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000',\n",
       " '-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we read the data in with `textFile()` we just have a set of strings separated by commas as our data. Let's split the data into separate entriees using the `map()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-122.230000',\n",
       "  '37.880000',\n",
       "  '41.000000',\n",
       "  '880.000000',\n",
       "  '129.000000',\n",
       "  '322.000000',\n",
       "  '126.000000',\n",
       "  '8.325200',\n",
       "  '452600.000000'],\n",
       " ['-122.220000',\n",
       "  '37.860000',\n",
       "  '21.000000',\n",
       "  '7099.000000',\n",
       "  '1106.000000',\n",
       "  '2401.000000',\n",
       "  '1138.000000',\n",
       "  '8.301400',\n",
       "  '358500.000000']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have something more closely resembling a collection of records. \n",
    "\n",
    "But, notice that the data does not have a header and is mostly unstructured. \n",
    "\n",
    "We can fix that by converting the data to a **DataFrame**. \n",
    "\n",
    "I have been told that n general DataFrames perform better than the RDDs, especially when using Python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n",
      "| households|housingMedianAge| latitude|  longitude|medianHouseValue|medianIncome| population|totalBedRooms| totalRooms|\n",
      "+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n",
      "| 126.000000|       41.000000|37.880000|-122.230000|   452600.000000|    8.325200| 322.000000|   129.000000| 880.000000|\n",
      "|1138.000000|       21.000000|37.860000|-122.220000|   358500.000000|    8.301400|2401.000000|  1106.000000|7099.000000|\n",
      "| 177.000000|       52.000000|37.850000|-122.240000|   352100.000000|    7.257400| 496.000000|   190.000000|1467.000000|\n",
      "| 219.000000|       52.000000|37.850000|-122.250000|   341300.000000|    5.643100| 558.000000|   235.000000|1274.000000|\n",
      "| 259.000000|       52.000000|37.850000|-122.250000|   342200.000000|    3.846200| 565.000000|   280.000000|1627.000000|\n",
      "| 193.000000|       52.000000|37.850000|-122.250000|   269700.000000|    4.036800| 413.000000|   213.000000| 919.000000|\n",
      "| 514.000000|       52.000000|37.840000|-122.250000|   299200.000000|    3.659100|1094.000000|   489.000000|2535.000000|\n",
      "| 647.000000|       52.000000|37.840000|-122.250000|   241400.000000|    3.120000|1157.000000|   687.000000|3104.000000|\n",
      "| 595.000000|       42.000000|37.840000|-122.260000|   226700.000000|    2.080400|1206.000000|   665.000000|2555.000000|\n",
      "| 714.000000|       52.000000|37.840000|-122.250000|   261100.000000|    3.691200|1551.000000|   707.000000|3549.000000|\n",
      "| 402.000000|       52.000000|37.850000|-122.260000|   281500.000000|    3.203100| 910.000000|   434.000000|2202.000000|\n",
      "| 734.000000|       52.000000|37.850000|-122.260000|   241800.000000|    3.270500|1504.000000|   752.000000|3503.000000|\n",
      "| 468.000000|       52.000000|37.850000|-122.260000|   213500.000000|    3.075000|1098.000000|   474.000000|2491.000000|\n",
      "| 174.000000|       52.000000|37.840000|-122.260000|   191300.000000|    2.673600| 345.000000|   191.000000| 696.000000|\n",
      "| 620.000000|       52.000000|37.850000|-122.260000|   159200.000000|    1.916700|1212.000000|   626.000000|2643.000000|\n",
      "| 264.000000|       50.000000|37.850000|-122.260000|   140000.000000|    2.125000| 697.000000|   283.000000|1120.000000|\n",
      "| 331.000000|       52.000000|37.850000|-122.270000|   152500.000000|    2.775000| 793.000000|   347.000000|1966.000000|\n",
      "| 303.000000|       52.000000|37.850000|-122.270000|   155500.000000|    2.120200| 648.000000|   293.000000|1228.000000|\n",
      "| 419.000000|       50.000000|37.840000|-122.260000|   158700.000000|    1.991100| 990.000000|   455.000000|2239.000000|\n",
      "| 275.000000|       52.000000|37.840000|-122.270000|   162900.000000|    2.603300| 690.000000|   298.000000|1503.000000|\n",
      "+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = rdd.map(lambda line: Row(longitude=line[0], \n",
    "                              latitude=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[7],\n",
    "                              medianHouseValue=line[8])).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine the data types associated with the dataframe `printSchemea()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- households: string (nullable = true)\n",
      " |-- housingMedianAge: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- medianHouseValue: string (nullable = true)\n",
      " |-- medianIncome: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- totalBedRooms: string (nullable = true)\n",
      " |-- totalRooms: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used the `textFile()` method to read our data in, the data types are all `string`\n",
    "The following would cast all of the columns to floats instead: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "df = df.withColumn(\"longitude\", df[\"longitude\"].cast(FloatType())) \\\n",
    "   .withColumn(\"latitude\", df[\"latitude\"].cast(FloatType())) \\\n",
    "   .withColumn(\"housingMedianAge\",df[\"housingMedianAge\"].cast(FloatType())) \\\n",
    "   .withColumn(\"totalRooms\", df[\"totalRooms\"].cast(FloatType())) \\ \n",
    "   .withColumn(\"totalBedRooms\", df[\"totalBedRooms\"].cast(FloatType())) \\ \n",
    "   .withColumn(\"population\", df[\"population\"].cast(FloatType())) \\ \n",
    "   .withColumn(\"households\", df[\"households\"].cast(FloatType())) \\ \n",
    "   .withColumn(\"medianIncome\", df[\"medianIncome\"].cast(FloatType())) \\ \n",
    "   .withColumn(\"medianHouseValue\", df[\"medianHouseValue\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that seems pretty inefficient, and it is. We can write a function to handle all of this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- households: float (nullable = true)\n",
      " |-- housingMedianAge: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- medianHouseValue: float (nullable = true)\n",
      " |-- medianIncome: float (nullable = true)\n",
      " |-- population: float (nullable = true)\n",
      " |-- totalBedRooms: float (nullable = true)\n",
      " |-- totalRooms: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def convertCols(df,names,dataType): #df - a dataframe, names - a list of col names, dataType - the cast conversion type\n",
    "    for name in names:\n",
    "        df = df.withColumn(name,df[name].cast(dataType))\n",
    "    return df\n",
    "\n",
    "names = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome',\\\n",
    "         'population', 'totalBedRooms', 'totalRooms']\n",
    "\n",
    "\n",
    "df = convertCols(df,names,FloatType())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|households|housingMedianAge|latitude|longitude|medianHouseValue|medianIncome|population|totalBedRooms|totalRooms|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|     126.0|            41.0|   37.88|  -122.23|        452600.0|      8.3252|     322.0|        129.0|     880.0|\n",
      "|    1138.0|            21.0|   37.86|  -122.22|        358500.0|      8.3014|    2401.0|       1106.0|    7099.0|\n",
      "|     177.0|            52.0|   37.85|  -122.24|        352100.0|      7.2574|     496.0|        190.0|    1467.0|\n",
      "|     219.0|            52.0|   37.85|  -122.25|        341300.0|      5.6431|     558.0|        235.0|    1274.0|\n",
      "|     259.0|            52.0|   37.85|  -122.25|        342200.0|      3.8462|     565.0|        280.0|    1627.0|\n",
      "|     193.0|            52.0|   37.85|  -122.25|        269700.0|      4.0368|     413.0|        213.0|     919.0|\n",
      "|     514.0|            52.0|   37.84|  -122.25|        299200.0|      3.6591|    1094.0|        489.0|    2535.0|\n",
      "|     647.0|            52.0|   37.84|  -122.25|        241400.0|        3.12|    1157.0|        687.0|    3104.0|\n",
      "|     595.0|            42.0|   37.84|  -122.26|        226700.0|      2.0804|    1206.0|        665.0|    2555.0|\n",
      "|     714.0|            52.0|   37.84|  -122.25|        261100.0|      3.6912|    1551.0|        707.0|    3549.0|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pyspark.sql` package has lots of convenient data exploration methods built in that support SQL query language execution. For example, we can select by columns:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|population|totalBedrooms|\n",
      "+----------+-------------+\n",
      "|     322.0|        129.0|\n",
      "|    2401.0|       1106.0|\n",
      "|     496.0|        190.0|\n",
      "|     558.0|        235.0|\n",
      "|     565.0|        280.0|\n",
      "|     413.0|        213.0|\n",
      "|    1094.0|        489.0|\n",
      "|    1157.0|        687.0|\n",
      "|    1206.0|        665.0|\n",
      "|    1551.0|        707.0|\n",
      "+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('population','totalBedrooms').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `filter()` method to perform a classic `SELECT FROM WHERE` query as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|population|totalBedrooms|\n",
      "+----------+-------------+\n",
      "|    2401.0|       1106.0|\n",
      "|    1157.0|        687.0|\n",
      "|    1206.0|        665.0|\n",
      "|    1551.0|        707.0|\n",
      "|    1504.0|        752.0|\n",
      "|    1212.0|        626.0|\n",
      "|    1015.0|        541.0|\n",
      "|    1258.0|        574.0|\n",
      "|    1377.0|        715.0|\n",
      "|    1959.0|        853.0|\n",
      "+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ndf = df.select('population','totalBedrooms').filter(df['totalBedrooms'] > 500)\n",
    "ndf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can get summary statistics pretty easilly too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|summary|       households|  housingMedianAge|         latitude|          longitude|  medianHouseValue|      medianIncome|        population|    totalBedRooms|        totalRooms|\n",
      "+-------+-----------------+------------------+-----------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|  count|            20640|             20640|            20640|              20640|             20640|             20640|             20640|            20640|             20640|\n",
      "|   mean|499.5396802325581|28.639486434108527|35.63186143109965|-119.56970444871473|206855.81690891474|3.8706710030346416|1425.4767441860465|537.8980135658915|2635.7630813953488|\n",
      "| stddev|382.3297528316098| 12.58555761211163|2.135952380602968|  2.003531742932898|115395.61587441359|1.8998217183639696|  1132.46212176534| 421.247905943133|2181.6152515827944|\n",
      "|    min|              1.0|               1.0|            32.54|            -124.35|           14999.0|            0.4999|               3.0|              1.0|               2.0|\n",
      "|    max|           6082.0|              52.0|            41.95|            -114.31|          500001.0|           15.0001|           35682.0|           6445.0|           39320.0|\n",
      "+-------+-----------------+------------------+-----------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick bit of feature engineering and transformation to optimize a linear regression on our feature set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|households|housingMedianAge|latitude|longitude|medianHouseValue|medianIncome|population|totalBedRooms|totalRooms|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|     126.0|            41.0|   37.88|  -122.23|        452600.0|      8.3252|     322.0|        129.0|     880.0|\n",
      "|    1138.0|            21.0|   37.86|  -122.22|        358500.0|      8.3014|    2401.0|       1106.0|    7099.0|\n",
      "|     177.0|            52.0|   37.85|  -122.24|        352100.0|      7.2574|     496.0|        190.0|    1467.0|\n",
      "|     219.0|            52.0|   37.85|  -122.25|        341300.0|      5.6431|     558.0|        235.0|    1274.0|\n",
      "|     259.0|            52.0|   37.85|  -122.25|        342200.0|      3.8462|     565.0|        280.0|    1627.0|\n",
      "|     193.0|            52.0|   37.85|  -122.25|        269700.0|      4.0368|     413.0|        213.0|     919.0|\n",
      "|     514.0|            52.0|   37.84|  -122.25|        299200.0|      3.6591|    1094.0|        489.0|    2535.0|\n",
      "|     647.0|            52.0|   37.84|  -122.25|        241400.0|        3.12|    1157.0|        687.0|    3104.0|\n",
      "|     595.0|            42.0|   37.84|  -122.26|        226700.0|      2.0804|    1206.0|        665.0|    2555.0|\n",
      "|     714.0|            52.0|   37.84|  -122.25|        261100.0|      3.6912|    1551.0|        707.0|    3549.0|\n",
      "|     402.0|            52.0|   37.85|  -122.26|        281500.0|      3.2031|     910.0|        434.0|    2202.0|\n",
      "|     734.0|            52.0|   37.85|  -122.26|        241800.0|      3.2705|    1504.0|        752.0|    3503.0|\n",
      "|     468.0|            52.0|   37.85|  -122.26|        213500.0|       3.075|    1098.0|        474.0|    2491.0|\n",
      "|     174.0|            52.0|   37.84|  -122.26|        191300.0|      2.6736|     345.0|        191.0|     696.0|\n",
      "|     620.0|            52.0|   37.85|  -122.26|        159200.0|      1.9167|    1212.0|        626.0|    2643.0|\n",
      "|     264.0|            50.0|   37.85|  -122.26|        140000.0|       2.125|     697.0|        283.0|    1120.0|\n",
      "|     331.0|            52.0|   37.85|  -122.27|        152500.0|       2.775|     793.0|        347.0|    1966.0|\n",
      "|     303.0|            52.0|   37.85|  -122.27|        155500.0|      2.1202|     648.0|        293.0|    1228.0|\n",
      "|     419.0|            50.0|   37.84|  -122.26|        158700.0|      1.9911|     990.0|        455.0|    2239.0|\n",
      "|     275.0|            52.0|   37.84|  -122.27|        162900.0|      2.6033|     690.0|        298.0|    1503.0|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|households|housingMedianAge|latitude|longitude|medianHouseValue|medianIncome|population|totalBedRooms|totalRooms|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|     126.0|            41.0|   37.88|  -122.23|           4.526|      8.3252|     322.0|        129.0|     880.0|\n",
      "|    1138.0|            21.0|   37.86|  -122.22|           3.585|      8.3014|    2401.0|       1106.0|    7099.0|\n",
      "|     177.0|            52.0|   37.85|  -122.24|           3.521|      7.2574|     496.0|        190.0|    1467.0|\n",
      "|     219.0|            52.0|   37.85|  -122.25|           3.413|      5.6431|     558.0|        235.0|    1274.0|\n",
      "|     259.0|            52.0|   37.85|  -122.25|           3.422|      3.8462|     565.0|        280.0|    1627.0|\n",
      "|     193.0|            52.0|   37.85|  -122.25|           2.697|      4.0368|     413.0|        213.0|     919.0|\n",
      "|     514.0|            52.0|   37.84|  -122.25|           2.992|      3.6591|    1094.0|        489.0|    2535.0|\n",
      "|     647.0|            52.0|   37.84|  -122.25|           2.414|        3.12|    1157.0|        687.0|    3104.0|\n",
      "|     595.0|            42.0|   37.84|  -122.26|           2.267|      2.0804|    1206.0|        665.0|    2555.0|\n",
      "|     714.0|            52.0|   37.84|  -122.25|           2.611|      3.6912|    1551.0|        707.0|    3549.0|\n",
      "|     402.0|            52.0|   37.85|  -122.26|           2.815|      3.2031|     910.0|        434.0|    2202.0|\n",
      "|     734.0|            52.0|   37.85|  -122.26|           2.418|      3.2705|    1504.0|        752.0|    3503.0|\n",
      "|     468.0|            52.0|   37.85|  -122.26|           2.135|       3.075|    1098.0|        474.0|    2491.0|\n",
      "|     174.0|            52.0|   37.84|  -122.26|           1.913|      2.6736|     345.0|        191.0|     696.0|\n",
      "|     620.0|            52.0|   37.85|  -122.26|           1.592|      1.9167|    1212.0|        626.0|    2643.0|\n",
      "|     264.0|            50.0|   37.85|  -122.26|             1.4|       2.125|     697.0|        283.0|    1120.0|\n",
      "|     331.0|            52.0|   37.85|  -122.27|           1.525|       2.775|     793.0|        347.0|    1966.0|\n",
      "|     303.0|            52.0|   37.85|  -122.27|           1.555|      2.1202|     648.0|        293.0|    1228.0|\n",
      "|     419.0|            50.0|   37.84|  -122.26|           1.587|      1.9911|     990.0|        455.0|    2239.0|\n",
      "|     275.0|            52.0|   37.84|  -122.27|           1.629|      2.6033|     690.0|        298.0|    1503.0|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import all from `sql.functions` \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Adjust the values of `medianHouseValue`\n",
    "df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the column of medianHouseValue in the above outputs to make sure that we transformed the data correctly. \n",
    "\n",
    "Let's do some more feature engineering and standardization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all from `sql.functions` if you haven't yet\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Divide `totalRooms` by `households`\n",
    "roomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n",
    "\n",
    "# Divide `population` by `households`\n",
    "populationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n",
    "\n",
    "# Divide `totalBedRooms` by `totalRooms`\n",
    "bedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "\n",
    "# Add the new columns to `df`\n",
    "df = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n",
    "   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n",
    "   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "   \n",
    "# Inspect the result\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're using the `col()` function to specify that we're using columnar data in our calculations. The `col(\"totalRooms\")/col(\"households\")` is acting like a numpy array, element wise dividing the results. \n",
    "\n",
    "Next we'll use the `select()` method to reorder the data so that our response variable is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-order and select columns\n",
    "df = df.select(\"medianHouseValue\", \n",
    "              \"totalBedRooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"medianIncome\", \n",
    "              \"roomsPerHousehold\", \n",
    "              \"populationPerHousehold\", \n",
    "              \"bedroomsPerRoom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to actually isolate the response variable of labels from the predictor variables using a DenseVector, which is essentially a `numpy` `ndarray`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import `DenseVector`\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define the `input_data` \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Replace `df` with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are all kinds of great machine learning algorithms and functions already built into PySpark in the Spark ML library. If you're interested in more data pipelining, try visiting this page: https://spark.apache.org/docs/latest/ml-pipeline.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264])),\n",
       " Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, 2.6851]))]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import `StandardScaler` \n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "scaled_df = scaler.transform(df)\n",
    "\n",
    "# Inspect the result\n",
    "scaled_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can divide the data into training and testing sets using the PySpark SQL `randomSplit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the regression model. The original tutorial directs you to the following URL for information on the linear regression model class: \n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.1340115638008952, 0.14999),\n",
       " (1.4485018834650096, 0.14999),\n",
       " (1.5713396046425587, 0.14999),\n",
       " (1.7496542762527307, 0.283),\n",
       " (1.2438468929500472, 0.366)]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predicted = linearModel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we can inspect the model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 0.0, 0.0, 0.2796, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coefficients for the model\n",
    "linearModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9841344205626824"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intercept for the model\n",
    "linearModel.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And summary data for the model is available as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8765335684459216"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the RMSE\n",
    "linearModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42282227755911483"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the R2\n",
    "linearModel.summary.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the Spark session..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On to Dask\n",
    "DASK was the acronym used to name the first computer ever built in Denmark in the 1950s\n",
    "\n",
    "**NOTE:**Not the first computer, just the first computer in Denmark\n",
    "\n",
    "\"Dask is a parallel programming library that combines with the Numeric Python ecosystem to provide parallel data frames, arrays, machine learning, and custom algorithms.\" - Dask Documentation\n",
    "\n",
    "\n",
    "## A few Differences from Spark \n",
    "\n",
    "1. Dask may be deployed on a cluster, but it is intended mostly for use on a single machine.\n",
    "    * Design focus was on efficient memory usage as opposed to ensuring reliability in the distributed environment\n",
    "2. Dask is intended to be smaller and more lightweight than Spark. \n",
    "    * Thus it offers less functionality, but is designed to play nice with other data science packages. \n",
    "3. Dask is written in Python, whereas Spark is written in Scala\n",
    "    * Debugging computational code in Spark is complicated given the added layers of the Java JVM and serialization layers \n",
    "    * Debugging computational code in Dask is just like Debugging any other Python computational code\n",
    "4. While Spark is built around RDDs, the main Dask data structure is a generic task graph with arbitrary data dependencies \n",
    "    * The low level, generic construction offers greater flexibility for developing more general computational algorithms \n",
    "    * Sited by Dask developers as the largest single difference between the two\n",
    "5. Task Graph Granularity\n",
    "    * Spark task graphs are a high level view of an application, which is interpreted at execution time then dispatched to compute nodes and performed in the distributed environment as \"many little tasks\"\n",
    "    * A Dask task graph is a low level view of the \"many little tasks\"\n",
    "    * As such, Dask is not optimized in the same way as Spark, but provides more transparency to the developer \n",
    "\n",
    "### Essentially\n",
    "If you are already working with Big Data hardware and using developing in Scala / Java and you want one software package to rule them all, Spark is your guy. \n",
    "\n",
    "If, on the other hand you are developing in Python using various Numeric Python libraries and you wish to add parallelism to already existing projects, the lightweight Dask may be more appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dask Essentials\n",
    "\n",
    "### Parallelism \n",
    "* Achieved primarilly through lazy function evaluation \n",
    "* Exposed to the programmer through the `@delayed` annotation on user defined functions or the `dask.delayed` function available in the library. \n",
    "* To evaluate the lazy values, use the `compute()` function\n",
    "\n",
    "### Dask Bag \n",
    "* Parallel list data structure for \"messy data\"\n",
    "* Data in the list can be mixed type, contain complex nested structures, missing values, etc. \n",
    "* Implement all of the typical functional API you're used to: `map`,`filter`,`groupby`, etc. \n",
    "* You can use the `take()` function to peak at the data you create with the above methods without using the `compute()` method\n",
    "* Bags provide very general computation - practically ANY python function\n",
    "* Limitations: \n",
    "    1. Bags **:** DataFrames **::** List **:** Numpy.ndarray\n",
    "    i.e. - **SLOW**\n",
    "    2. `Bag.groupby` is particularly slow, and its replacement, `Bag.foldby`, is confusing\n",
    "  \n",
    "### Distributed Dask \n",
    "* I'm not focusing on this, refer to the Dask tutorial...\n",
    "\n",
    "### Dask Arrays\n",
    "* Essentially distributed Numpy\n",
    "* Employ parallel computation using all of your computer cores\n",
    "* Effectively stream larger than memory data from disk \n",
    "* Q: How long does it take to sum 1Billion Numbers? \n",
    "    * [A]: around 5 seconds...\n",
    "* A large set of the Numpy API has been implemented \n",
    "* Limitations: \n",
    "    * np.linalg is not yet implemented\n",
    "    * fancy indexing and operations like `np.where()`\n",
    "    * no sorting! \n",
    "\n",
    "### Dataframes\n",
    "* A blocked, parallel dataframe object which mimics the Pandas dataframe to a large degree\n",
    "* Actually constructed of several independent Pandas dataframes smooshed together\n",
    "* Distributed Dask allows you to use dataframes across a clutster\n",
    "* Whenever Pandas releases the Global Interpreter Lock (GIL) Dask can run several Pandas operations in parallel\n",
    "    * SpeedUp on the order of the number of cores\n",
    "* Limitations: \n",
    "    * Pandas is more mature and fully functional \n",
    "        * Non-trivial bugs are frequently reported in Dask dataframes (quickly fixed... :/ )\n",
    "        * If your data fits in memory, use Pandas\n",
    "    * Only a small subset of Pandas is currently implemented\n",
    "    * \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
